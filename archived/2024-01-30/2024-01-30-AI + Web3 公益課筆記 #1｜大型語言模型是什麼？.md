---
layout: post
title: "AI + Web3 公益課筆記 #1｜大型語言模型是什麼？"
date: 2024-01-30T11:57:44.000Z
author: 閱讀筆耕
from: https://matters.news/@penfarming/ai-web3-%E5%85%AC%E7%9B%8A%E8%AA%B2%E7%AD%86%E8%A8%98-1-%E5%A4%A7%E5%9E%8B%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%98%AF%E4%BB%80%E9%BA%BC-bafybeia32tconosnjoh2p7g77fai47zfbsmm6mikfqkjw7ighu7strlfuq
tags: [ Matters ]
comments: True
categories: [ Matters ]
---
<!--1706615864000-->
[AI + Web3 公益課筆記 #1｜大型語言模型是什麼？](https://matters.news/@penfarming/ai-web3-%E5%85%AC%E7%9B%8A%E8%AA%B2%E7%AD%86%E8%A8%98-1-%E5%A4%A7%E5%9E%8B%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%98%AF%E4%BB%80%E9%BA%BC-bafybeia32tconosnjoh2p7g77fai47zfbsmm6mikfqkjw7ighu7strlfuq)
------

<div>
<p>2024.01.28</p><p>大家好！我使用了「節錄評論法」來寫下 <a target="_blank" rel="noopener noreferrer nofollow" href="https://matters.town/@wenqian/508029-%E5%B9%B4%E6%9C%AB%E7%A6%8F%E5%88%A9-%E9%80%9A%E5%BE%80%E4%B8%96%E7%95%8C%E7%9A%84%E8%B7%AF-ai-web3%E5%85%AC%E7%9B%8A%E8%AA%B2-bafybeig5xdwngs3uwu3cxemqeurgek23dz6s4qkd5pklobzijvdb2xt5fq">AI + Web3 公益課</a> 的筆記。這是一種讓筆記充滿活力，可以「和自己對話」的整理方式。</p><p>操作方式是，摘錄「印象深刻的段落和句子」或「令人困惑的段落」，並且「用自己的話發表看法」（in my opinion, IMO），甚至更進一步「與自己的經驗產生連結」。</p><p>讓我們馬上開始吧！</p><figure class="image"><img src="https://imagedelivery.net/kDRCweMmqLnTPNlbum-pYA/prod/embed/deceffbf-4ef9-401c-8c62-7cede58587c8.png/public" referrerpolicy="no-referrer"><figcaption>AI + Web3 公益課</figcaption></figure><hr><h2>▇  <strong>開場嘉賓致詞</strong></h2><ul><li><p>Web3 和 Web2 很不同的是，它是去中心化的，妥善處理了社區平台「保存、隱私、信任、<strong>價值確權、價值分配</strong>」等問題。</p></li><li><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://app.anuniverse.com/">AU Universe</a> 的願景是「<strong>將知識的價值回歸給創造知識的人</strong>」，在這裡發布的每一條訊息都會被你的數字分身（基於 AI 技術產生的 Avatar ）學習，並且產生價值。</p></li></ul><hr><h2>▇  課綱（講師：陳財貓）</h2><figure class="image"><img src="https://imagedelivery.net/kDRCweMmqLnTPNlbum-pYA/prod/embed/9b2671ee-420f-4776-8854-06bff732e933.png/public" referrerpolicy="no-referrer"><figcaption>課綱</figcaption></figure><h2>▇  大語言模型是什麼？</h2><h3>▍<strong>大語言模型是什麼？接龍機器</strong></h3><ul><li><p>大語言模型（Large Language Model, LLM）可理解為一種「預測下一個詞元（token）」的統計模型</p><ul><li><p>白話說 → <strong>LLM 是一個「接龍機器」</strong>。<br class="smart">　</p></li></ul></li></ul><h3>▍<strong>GPT 是什麼？從個別單字理解</strong></h3><ul><li><p>GPT（Generative pre-trained transformer）是一種「生成式預訓練模型」。</p></li><li><p><strong>Pre-trained</strong>：基於海量數據上的「預訓練」。</p><ul><li><p>白話說 → GPT 學富五車。</p></li></ul></li><li><p><strong>Generative</strong>：基於海量數據上的機率分佈「生成」新的數據。</p><ul><li><p>白話說 → GPT 能連貫文本。</p></li></ul></li><li><p><strong>Transformer</strong>：一種深度學習模型採用的「架構」，讓模型能理解我們輸入的文本（ 一連串序列）。</p><ul><li><p>白話說 → GPT 能捕捉到字裡行間行間的依賴、從屬、因果關係等。<br class="smart">　</p></li></ul></li></ul><h3>▍Chat<strong>GPT 是什麼？</strong></h3><ul><li><p>ChatGPT 是 GPT 的其中一種版本，是一種應用產品（聊天機器人介面）。</p></li></ul><p><strong>【閱讀筆耕 IMO 🙋】何謂無情</strong></p><p>挺喜歡 LLM 是「接龍機器」這樣的類比，而且 LLM 是「無情」的。</p><p>針對課綱中的「無情」一詞，我的理解是 LLM 雖然能夠不斷預測文字，<strong>但他並不能真正理解那些文字背後所傳達的意思</strong>，所以他是不帶感情，理性地從機率分佈中生成下一個 Token。</p><hr><h2>▇  GPT 可以用來做什麼？</h2><h3>▍執行自然語言處理任務</h3><ul><li><p>生成式任務：例如寫文章、寫詩歌、寫程式碼⋯⋯。</p></li><li><p>情感分析：例如餵食大量產品評論、新聞標題等人們的數位足跡，讓 GPT 研判並預測大環境的下一步。</p></li><li><p>文本校正</p></li><li><p>文本摘要</p></li><li><p>聊天：角色設定可以是朋友、情人、專家⋯⋯甚至是神明（AI 佛祖、AI 耶穌）。　</p></li></ul><h3>▍選單（menu）翻譯產品</h3><figure class="image"><img src="https://imagedelivery.net/kDRCweMmqLnTPNlbum-pYA/prod/embed/3899c14c-4c07-4225-8dac-6ca55cd1cb8f.png/public" referrerpolicy="no-referrer"><figcaption></figcaption></figure><h3>▍智能增強</h3><ul><li><p>調用 GPT 學富五車的知識來服務目標群眾。</p></li><li><p>窮盡一生，沒有任何一個人可以讀完這些來自於人類歷史上的精華——但是 GPT 卻可以，畢竟「預訓練」的數據量極大。</p></li></ul><p><strong>【閱讀筆耕 IMO 🙋】AI 讓心智腳踏車進化</strong></p><p>人類非常擅長製造工具，工具是人類肢體和感官的延伸，例如腳踏車增加了人類（的雙足）移動的效率。</p><blockquote><p>賈伯斯曾在一場專訪中說道：「對我來說，電腦是人類歷來發明的最重要工具，從此讓我們的心智騎上腳踏車。」</p></blockquote><p>而當這台電腦還嵌上 AI 功能時，這台車進化了，甚至不用我們親自去踩踏，也能夠引領我們來到更遠的地方。</p><p>　　</p><h3>▍工作外包</h3><p>複雜的問題簡單化，簡單的問題<strong>流程化。</strong></p><blockquote><p>Nevertheless, there is merit to the claim that much problem solving effort is directed at structuring problems, and only a fraction of it at solving problems once they are structured.</p><p>——Herbert A. Simon<br class="smart">⠀<br class="smart">大部分的問題解決努力，都集中在為問題構建結構上，而對於已經結構化的問題，實際解決它們只佔了一小部分努力。<br class="smart">——赫伯特・西蒙</p></blockquote><p>一但我們能把某些工作的具體場景「抽象化」成最核心的底層邏輯，那麼就有了可以外包（給別人做、給 AI 做）出去的本錢。</p><p>例如，把「設計提示（prompt）」這件事情，拆解為五個最根本的元素，而且它們之間具有可以畫出流程圖的關係，分發給 AI 來打理。</p><figure class="image"><img src="https://imagedelivery.net/kDRCweMmqLnTPNlbum-pYA/prod/embed/a0f5ed62-9be8-4eeb-b651-49978522a6be.png/public" referrerpolicy="no-referrer"><figcaption></figcaption></figure><p><strong>【閱讀筆耕 IMO 🙋】外包前，先把「系統」剝到最乾淨</strong></p><p>我聯想到《<a target="_blank" rel="noopener noreferrer nofollow" href="https://matters.town/@penfarming/481386-imo-%E4%BA%BA%E7%89%A9%E8%AA%8C-%E6%99%AE%E9%80%9A%E4%BA%BA%E7%9A%84%E8%B2%A1%E5%8B%99%E8%87%AA%E7%94%B1%E4%B9%8B%E9%81%93-%E8%AA%AA%E6%9B%B8%E5%88%86%E4%BA%AB%E6%9C%83-bafybeiblbcocrdwnwqt22jb5peu52rypvxzgkvvcarkrdxj2qp3fgoykya">普通人的財富自由之道</a>》裡提過工作外包的先決條件，稱之為「系統的植入」。不管你有沒有想要外包某一份工作，都先把系統給架設好：</p><ol><li><p>寫下你在一週內做的所有事情。</p></li><li><p>將工作分為清單一（重複執行的工作）與清單二（一次性任務），然後丟掉清單二。</p></li><li><p>重新排列清單一，從最耗時到最不耗時。逐條找出你想要為其建立系統的任務。</p></li><li><p>寫出你是怎麼完成該任務的步驟動線。然後，確認流程，看看是否能找到任何不必要的步驟，<strong>先刪除所有不必要的步驟，然後才做優化</strong>，直到擁有你所能建立的最精簡和最高效的流程。</p></li><li><p>在你進行這段流程時創作一段「說明書」⋯⋯逐步累積起一套培訓內容。</p></li></ol><p>這麽做的好處是，未來如果要招募夥伴，<strong>你會知道哪些事情可以外包？哪種人才是最優先的？而哪些事情是非我不可，沒有其他人可以接手？</strong>對接時，新成員就可以依循先前建置好的「說明書」很快地進入狀況。</p><p>同時，<strong>這些「說明書」也可以成為一再重複利用的模板</strong>，不斷迭代這套 SOP。</p><p>　</p><h3>▍湧現能力（Emergent Abilities）</h3><ul><li><p>在模型變大到一定程度時，出現了一些新的特性、能力或行為。例如思維鏈（Chain-of-Thought）多步推理能力。</p><ul><li><p>白話說 → 是「從量變到質變」，是「整體大於部分的總和」。</p></li></ul></li></ul><p><strong>【閱讀筆耕 IMO 🙋】知識的點、線、面串起「湧現」</strong></p><p>我覺得用「知識點線面」來理解何謂湧現能力，是很不錯的。</p><p>當餵食給模型的知識點少少的時候，這些知識點彼此是「孤島」，還沒有辦法產生交互作用。就像是一幅「只有Ａ、Ｂ兩個節點的地圖」一樣，能做的事情很有限。</p><p>但是當知識點的數量多到一定程度時，這些知識點已經形成了一個「體系」，這時候模型已經具備舉一反三的能力。想像是一幅「記載著密密麻麻交通節點、路線的地圖」，你只是問他從Ａ點到Ｂ點應該怎麼走，他可以給你不止一種解答，還為你比較多種方案的成本效益分析。</p><p><strong>知識的點、線、面串起神經網路，交織成一個體系帶來 1+1 >2 化學效應，就是湧現。</strong></p><hr><h2>▇  GPT 的不足與缺陷</h2><blockquote><p>GPT 有一個秘密，他其實是一個失憶症患者，為了不讓別人發現他的秘密，他把和別人的對話寫在一本日記本上；每次和別人說話之前，GPT 都會先翻閱一下日記本，回顧之前的對話，然後才做回應。</p></blockquote><h3>▍<strong>有限的上下文窗口（Context Window）</strong></h3><ul><li><p><strong>GPT 會「忘記」聊天中太早的內容</strong></p><ul><li><p>知識點 → 因為「日記本」的容量是有限的。</p></li></ul></li><li><p><strong>不相關的話題最好在不同對話裡聊</strong></p><ul><li><p>知識點 → GPT 是會一次讀入所有對話內容，再做出反應的。</p></li><li><p>知識點 → 所以 GPT 仍然會考量早期的訊息（只要還被「日記本」所涵蓋到），如果新、舊訊息彼此不相關的話，會變成一種雜訊，一種干擾。</p></li></ul></li><li><p><strong>我們無法「訓練」GPT</strong></p><ul><li><p>知識點 → 訓練是一種特定過程，涉及模型參數調整，<strong>只有 OpenAI 能執行</strong>。</p></li><li><p>知識點 → 我們覺得 GPT 愈來愈聰明，其實是因為他的「日記本」冊數更多、內容更豐富、讓他對背景訊息的掌握度更高的關係。</p></li></ul></li></ul><p><strong>【閱讀筆耕 IMO 🙋】關閉訓練，保護隱私</strong></p><p>點擊 GPT 聊天界面左下角頭像，進入「設定 ▷ 數據控制 ▷ 聊天歷史與訓練」選單，預設是開啟的。我們可以關閉這個功能，讓 GPT 不再紀錄聊天訊息，不允許 OpenAI 以我們的對話來訓練模型，這麼做更能保護隱私。</p><figure class="image"><img src="https://imagedelivery.net/kDRCweMmqLnTPNlbum-pYA/prod/embed/72ff1d59-6223-4644-ab41-4cf781c93761.png/public" referrerpolicy="no-referrer"><figcaption>設定 ▷ 數據控制 ▷ 聊天歷史與訓練</figcaption></figure><figure class="image"><img src="https://imagedelivery.net/kDRCweMmqLnTPNlbum-pYA/prod/embed/41108e22-7e17-40f5-802b-927064375911.png/public" referrerpolicy="no-referrer"><figcaption>不允許 OpenAI 以我們的對話來訓練模型</figcaption></figure><h3>▍<strong>過時的數據</strong></h3><ul><li><p>透過付費升級把 GPT 從 3.5 升級到 4.0，或是安裝外掛插件，可以大幅緩解這個缺失。<br class="smart">　</p></li></ul><h3>▍<strong>幻覺（</strong>hallucination<strong>）</strong></h3><ul><li><p>GPT 編造不存在的東西與事實。</p><ul><li><p>白話說 → 一本正經的胡說八道。</p></li></ul></li></ul><p><strong>【閱讀筆耕 IMO 🙋】不花錢，也能釋放 GPT 的能力</strong></p><p>關於「過時數據」與「幻覺」這兩個問題是息息相關的。如果問 GPT 時事類問題，當他的數據庫太舊，導致他查不到正確答案時，他就會畫虎爛。</p><p>我之前使用一款名為「<a target="_blank" rel="noopener noreferrer nofollow" href="https://chromewebstore.google.com/detail/webchatgpt-chatgpt-%E5%85%B7%E5%82%99%E4%BA%92%E8%81%AF%E7%B6%B2%E8%A8%AA/lpfemeioodjbpieminkklglpmhlngfcn?hl=zh-TW">WebChatGPT</a>」的瀏覽器插件，讓即使是沒有付費升級的 GPT 3.5，也能在網路上搜尋資料，就可以有效改善這個缺失。</p><p>以下是用「<a target="_blank" rel="noopener noreferrer nofollow" href="https://www.ctwant.com/article/269571">白飯之亂</a>」新聞事件實測的結果：</p><figure class="image"><img src="https://imagedelivery.net/kDRCweMmqLnTPNlbum-pYA/prod/embed/516e1d3f-57a1-464d-aee6-6bacb1e369d5.png/public" referrerpolicy="no-referrer"><figcaption>before</figcaption></figure><figure class="image"><img src="https://imagedelivery.net/kDRCweMmqLnTPNlbum-pYA/prod/embed/e6f24816-4df2-406a-a54e-31fb0f020a4a.png/public" referrerpolicy="no-referrer"><figcaption>after</figcaption></figure><hr><blockquote><p>🌱 免費訂閱【<strong><a target="_blank" rel="noopener noreferrer nofollow" href="https://creatoreconomyimo.substack.com/">創作者經濟 IMO</a></strong>】電子報。<br class="smart">電子報是以 Heptabase 編輯，<a target="_blank" rel="noopener noreferrer nofollow" href="https://get.heptabase.com/imo">免費試用 7 天</a>，和我們一起寫下 IMO。</p></blockquote><blockquote><p>🌱 加入<strong>【<a target="_blank" rel="noopener noreferrer nofollow" href="https://matters.news/~penfarming">中書神經系統</a>】</strong>圍爐，專題 <a target="_blank" rel="noopener noreferrer nofollow" href="https://matters.news/@penfarming/%E9%96%B1%E8%AE%80%E7%AD%86%E8%80%95-2021-%E5%9C%8D%E7%88%90%E6%AA%A2%E8%A8%8E-2022-%E5%B1%95%E6%9C%9B%E8%88%87%E5%AF%A6%E8%B8%90-bafyreibgwdmctf2bmhq4bufkyuepmgkvh5mhsxxm5bbjnltdw2b5h2jnsy">#寫作的反思</a> 與 <a target="_blank" rel="noopener noreferrer nofollow" href="https://matters.news/tags/VGFnOjcwNTk2">#爐內真心話</a> 連載中。</p></blockquote><blockquote><p>🌱 我在其它平台出沒【<strong><a target="_blank" rel="noopener noreferrer nofollow" href="https://www.facebook.com/penfarming">Meta</a></strong>｜<strong><a target="_blank" rel="noopener noreferrer nofollow" href="https://twitter.com/leo7283">X</a></strong>｜<strong><a target="_blank" rel="noopener noreferrer nofollow" href="https://liker.social/@Penfarning">Liker Social</a></strong>｜<strong><a target="_blank" rel="noopener noreferrer nofollow" href="https://matters.news/@penfarming">Matters</a></strong>｜<strong><a target="_blank" rel="noopener noreferrer nofollow" href="https://medium.com/%E9%96%B1%E8%AE%80%E7%AD%86%E8%80%95">Medium</a></strong>｜<strong><a target="_blank" rel="noopener noreferrer nofollow" href="https://vocus.cc/penfarming/home">vocus</a></strong> 】<br class="smart"><a target="_blank" rel="noopener noreferrer nofollow" href="mailto:%E5%90%88%E4%BD%9C%E8%81%AF%E7%B9%AB%EF%BC%9Apenfarming.writer@gmail.com">合作聯繫：penfarming.writer@gmail.com</a></p></blockquote><blockquote><p>🌱 我的教學文與邀請連結</p><p><strong>註冊幣安</strong>｜<a target="_blank" rel="noopener noreferrer nofollow" href="https://matters.town/@penfarming/442941-%E6%96%B0%E8%88%8A%E6%88%B6%E9%83%BD%E6%9C%89-%E5%B9%A3%E5%AE%89%E6%8A%BD-%E9%9B%BB%E5%8B%95%E6%BB%91%E6%9D%BF%E8%BB%8A-%E9%99%AA%E4%BD%A0%E7%A9%BF%E8%B6%8A%E7%89%9B%E7%86%8A-ft-%E5%B9%A3%E5%AE%89%E9%96%8B%E6%88%B6%E6%95%99%E5%AD%B8-bafybeifkgprrruqdsnnccfcrtcgvr4uqjguxgy44cl4hhl75txqhol5fyu">幣安開戶＋實名認證教學</a>。<strong><a target="_blank" rel="noopener noreferrer nofollow" href="https://presearch.org/signup?rid=3090491"><br class="smart"></a>註冊 Presearch </strong>｜<a target="_blank" rel="noopener noreferrer nofollow" href="https://matters.news/@penfarming/%E9%96%B1%E8%AE%80%E7%AD%86%E8%80%95-%E4%B8%80%E8%88%89%E4%B8%89%E5%BE%97%E7%9A%84-presearch-%E4%BD%BF%E7%94%A8%E5%BF%83%E5%BE%97-%E9%99%84%E6%95%99%E5%AD%B8-bafyreiec7mjtpjcy6bre3fgn7k53pmapqtnbziksneu6pamuvjqnbhcg5y">一舉三得的 search to earn 使用心得</a>。</p></blockquote>
</div>
